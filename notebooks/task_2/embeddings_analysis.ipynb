{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from utils.config import*\n",
    "from src.task_2.helpers import*\n",
    "from src.task_2.tokenizer import*\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data + config\n",
    "config = load_config()\n",
    "seed = config['seed']\n",
    "data = load_file('data/glycan_embedding/df_glycan.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "# TODO uniformize naming conventions between the two models\n",
    "sweetnet_config = config['models']['sweetnet']\n",
    "roberta_config = config['models']['roberta']\n",
    "sweetnet = load_model(sweetnet_config['training']['save_dir'] + '/Sweetnet_Family.pt', 'SweetNet', config=sweetnet_config)\n",
    "roberta = load_model(roberta_config['training']['output_dir'], 'RoBERTa', config=roberta_config)\n",
    "# Roberta tokenizer\n",
    "wrapper = HuggingFaceTokenizerWrapper()\n",
    "wrapper.load(roberta_config['tokenizer']['path'])\n",
    "tokenizer = wrapper.get_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encs = tokenizer(data['glycan'].tolist(), padding='max_length', max_length=tokenizer.model_max_length, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = {'input_ids' : encs['input_ids'][0].unsqueeze(0).to(roberta.device),\\\n",
    "              'attention_mask' : encs['attention_mask'][0].unsqueeze(0).to(roberta.device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = roberta(**data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the last hidden state\n",
    "embedding = output.hidden_states[-1].cpu().numpy()\n",
    "attention_mask = data_test['attention_mask'].unsqueeze(-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embedding*attention_mask).sum(axis=1).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_mask = (attention_mask.sum(axis=1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(12)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sentence embedding from mask word embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embeddings(data: pd.DataFrame, model, tokenizer=None, save_path=None) -> pd.DataFrame: \n",
    "\n",
    "    from glycowork.ml.models import SweetNet\n",
    "    from glycowork.ml.inference import glycans_to_emb\n",
    "    from transformers import RobertaForMaskedLM\n",
    "\n",
    "    assert('glycan' in data.columns)\n",
    "\n",
    "    if isinstance(model, SweetNet):\n",
    "        embeddings = glycans_to_emb(data['glycan'].values, model)\n",
    "        if save_path:\n",
    "            dt = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "            with open(os.path.join(save_path, f'embeddings_{dt}.pkl'), 'wb') as f:\n",
    "                pickle.dump(embeddings, f)\n",
    "        return embeddings\n",
    "    \n",
    "    elif isinstance(model, RobertaForMaskedLM):\n",
    "\n",
    "        assert(tokenizer is not None)\n",
    "\n",
    "        errors_g, embeddings = [], []\n",
    "        encodings = tokenizer(data['glycan'].tolist(),\\\n",
    "                              truncation=True,\\\n",
    "                              padding='max_length',\\\n",
    "                              max_length=tokenizer.model_max_length,\\\n",
    "                             return_tensors='pt')\n",
    "        encodings = {k: v.to(DEVICE) for k, v in encodings.items()}\n",
    "\n",
    "        for g, encoding in tqdm(zip(data['glycan'].values, encodings['input_ids'].tolist())):\n",
    "            try : \n",
    "                with torch.no_grad():\n",
    "                    embed = model(**encoding)\n",
    "                last_hidden_state = embed[-1][-1]\n",
    "                # Average token embedding to build sequence embedding\n",
    "                glycan_embed = last_hidden_state.squeeze(0).mean(dim=0).numpy()\n",
    "                embeddings.append(glycan_embed)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {g} : {e}\")\n",
    "                errors_g.append(g)\n",
    "        \n",
    "        if save_path:\n",
    "            dt = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "            with open(os.path.join(save_path, f'embeddings_{dt}.pkl'), 'wb') as f:\n",
    "                pickle.dump(embeddings, f)\n",
    "        \n",
    "        return embeddings, errors_g\n",
    "    else:\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mismatching brackets in formatted glycan string.\n",
      "Warning: Mismatching brackets in formatted glycan string.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('models/embeddings/SweetNet', exist_ok=True)\n",
    "embeddings_sweetnet = get_embeddings(data, sweetnet, save_path='models/embeddings/SweetNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.239049</td>\n",
       "      <td>-1.205554</td>\n",
       "      <td>2.505064</td>\n",
       "      <td>-2.204096</td>\n",
       "      <td>-1.664683</td>\n",
       "      <td>-1.788511</td>\n",
       "      <td>-1.561113</td>\n",
       "      <td>-0.461428</td>\n",
       "      <td>-0.637427</td>\n",
       "      <td>-0.611985</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.380926</td>\n",
       "      <td>1.291588</td>\n",
       "      <td>3.231824</td>\n",
       "      <td>-1.089608</td>\n",
       "      <td>-0.278298</td>\n",
       "      <td>-1.629229</td>\n",
       "      <td>0.745970</td>\n",
       "      <td>-1.162513</td>\n",
       "      <td>-0.956747</td>\n",
       "      <td>-0.602077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.528192</td>\n",
       "      <td>-1.131496</td>\n",
       "      <td>1.589969</td>\n",
       "      <td>-1.757410</td>\n",
       "      <td>-0.918702</td>\n",
       "      <td>-0.714965</td>\n",
       "      <td>-0.662135</td>\n",
       "      <td>-1.089974</td>\n",
       "      <td>-0.566921</td>\n",
       "      <td>-0.317467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.387416</td>\n",
       "      <td>1.024946</td>\n",
       "      <td>2.244558</td>\n",
       "      <td>-0.733578</td>\n",
       "      <td>-1.347370</td>\n",
       "      <td>-1.255779</td>\n",
       "      <td>-0.328468</td>\n",
       "      <td>2.809101</td>\n",
       "      <td>-0.764077</td>\n",
       "      <td>2.894966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.133796</td>\n",
       "      <td>-0.015784</td>\n",
       "      <td>-0.860791</td>\n",
       "      <td>-0.323467</td>\n",
       "      <td>-0.193998</td>\n",
       "      <td>-0.250146</td>\n",
       "      <td>-0.160487</td>\n",
       "      <td>-0.072700</td>\n",
       "      <td>-0.512950</td>\n",
       "      <td>0.287123</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.352306</td>\n",
       "      <td>1.462230</td>\n",
       "      <td>0.445271</td>\n",
       "      <td>-0.385861</td>\n",
       "      <td>-0.960686</td>\n",
       "      <td>-0.577423</td>\n",
       "      <td>-0.462516</td>\n",
       "      <td>-1.020753</td>\n",
       "      <td>-0.526767</td>\n",
       "      <td>2.492939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024131</td>\n",
       "      <td>-0.798205</td>\n",
       "      <td>1.159252</td>\n",
       "      <td>-0.872954</td>\n",
       "      <td>0.882341</td>\n",
       "      <td>-0.346515</td>\n",
       "      <td>-1.095960</td>\n",
       "      <td>0.972724</td>\n",
       "      <td>-1.133452</td>\n",
       "      <td>-1.039080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089598</td>\n",
       "      <td>-0.750147</td>\n",
       "      <td>1.849371</td>\n",
       "      <td>-1.229478</td>\n",
       "      <td>-1.216052</td>\n",
       "      <td>-0.970517</td>\n",
       "      <td>-0.432945</td>\n",
       "      <td>1.118007</td>\n",
       "      <td>-0.068015</td>\n",
       "      <td>-0.280603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.293180</td>\n",
       "      <td>-1.272951</td>\n",
       "      <td>-0.727258</td>\n",
       "      <td>-0.122571</td>\n",
       "      <td>-0.034667</td>\n",
       "      <td>1.133042</td>\n",
       "      <td>-0.651254</td>\n",
       "      <td>2.912364</td>\n",
       "      <td>-0.884565</td>\n",
       "      <td>0.425860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362923</td>\n",
       "      <td>0.245477</td>\n",
       "      <td>0.673627</td>\n",
       "      <td>-1.123325</td>\n",
       "      <td>-0.799653</td>\n",
       "      <td>-1.161224</td>\n",
       "      <td>0.962933</td>\n",
       "      <td>-0.421276</td>\n",
       "      <td>-1.176906</td>\n",
       "      <td>0.747134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50584</th>\n",
       "      <td>0.182961</td>\n",
       "      <td>2.211320</td>\n",
       "      <td>1.805225</td>\n",
       "      <td>1.480693</td>\n",
       "      <td>1.445918</td>\n",
       "      <td>0.361411</td>\n",
       "      <td>-0.356336</td>\n",
       "      <td>0.911873</td>\n",
       "      <td>-0.327498</td>\n",
       "      <td>1.320318</td>\n",
       "      <td>...</td>\n",
       "      <td>1.688902</td>\n",
       "      <td>1.070147</td>\n",
       "      <td>1.047303</td>\n",
       "      <td>1.543294</td>\n",
       "      <td>0.785552</td>\n",
       "      <td>1.175742</td>\n",
       "      <td>1.371868</td>\n",
       "      <td>1.143094</td>\n",
       "      <td>0.849715</td>\n",
       "      <td>0.742523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50585</th>\n",
       "      <td>0.357590</td>\n",
       "      <td>0.946273</td>\n",
       "      <td>1.268209</td>\n",
       "      <td>0.283762</td>\n",
       "      <td>3.105134</td>\n",
       "      <td>2.102657</td>\n",
       "      <td>-0.898391</td>\n",
       "      <td>1.466275</td>\n",
       "      <td>-0.227159</td>\n",
       "      <td>1.035146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801446</td>\n",
       "      <td>0.807440</td>\n",
       "      <td>1.796256</td>\n",
       "      <td>0.166234</td>\n",
       "      <td>-0.323527</td>\n",
       "      <td>0.401960</td>\n",
       "      <td>1.001705</td>\n",
       "      <td>0.834673</td>\n",
       "      <td>0.467638</td>\n",
       "      <td>0.175889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50586</th>\n",
       "      <td>0.230199</td>\n",
       "      <td>1.042811</td>\n",
       "      <td>1.529540</td>\n",
       "      <td>0.703689</td>\n",
       "      <td>2.225735</td>\n",
       "      <td>1.142767</td>\n",
       "      <td>-0.695822</td>\n",
       "      <td>1.781696</td>\n",
       "      <td>0.190462</td>\n",
       "      <td>0.516006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951339</td>\n",
       "      <td>0.521067</td>\n",
       "      <td>1.875985</td>\n",
       "      <td>0.725284</td>\n",
       "      <td>0.020722</td>\n",
       "      <td>0.748264</td>\n",
       "      <td>0.863091</td>\n",
       "      <td>0.745802</td>\n",
       "      <td>0.791014</td>\n",
       "      <td>0.215019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50587</th>\n",
       "      <td>-0.028394</td>\n",
       "      <td>1.833864</td>\n",
       "      <td>1.221431</td>\n",
       "      <td>0.805910</td>\n",
       "      <td>1.913851</td>\n",
       "      <td>0.948037</td>\n",
       "      <td>-0.490566</td>\n",
       "      <td>1.633394</td>\n",
       "      <td>-0.326149</td>\n",
       "      <td>-0.071438</td>\n",
       "      <td>...</td>\n",
       "      <td>1.729324</td>\n",
       "      <td>1.294404</td>\n",
       "      <td>1.559716</td>\n",
       "      <td>1.259737</td>\n",
       "      <td>0.920063</td>\n",
       "      <td>1.105738</td>\n",
       "      <td>0.657258</td>\n",
       "      <td>1.619102</td>\n",
       "      <td>0.777057</td>\n",
       "      <td>0.887049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50588</th>\n",
       "      <td>-0.438666</td>\n",
       "      <td>2.606170</td>\n",
       "      <td>1.740339</td>\n",
       "      <td>0.983071</td>\n",
       "      <td>1.565430</td>\n",
       "      <td>1.317871</td>\n",
       "      <td>-0.903871</td>\n",
       "      <td>0.867624</td>\n",
       "      <td>-0.268109</td>\n",
       "      <td>2.056823</td>\n",
       "      <td>...</td>\n",
       "      <td>1.597416</td>\n",
       "      <td>1.730113</td>\n",
       "      <td>1.182213</td>\n",
       "      <td>1.604889</td>\n",
       "      <td>1.089531</td>\n",
       "      <td>0.878835</td>\n",
       "      <td>2.072124</td>\n",
       "      <td>1.072981</td>\n",
       "      <td>0.623154</td>\n",
       "      <td>1.233232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50589 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -0.239049 -1.205554  2.505064 -2.204096 -1.664683 -1.788511 -1.561113   \n",
       "1     -0.528192 -1.131496  1.589969 -1.757410 -0.918702 -0.714965 -0.662135   \n",
       "2      2.133796 -0.015784 -0.860791 -0.323467 -0.193998 -0.250146 -0.160487   \n",
       "3      0.024131 -0.798205  1.159252 -0.872954  0.882341 -0.346515 -1.095960   \n",
       "4     -0.293180 -1.272951 -0.727258 -0.122571 -0.034667  1.133042 -0.651254   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "50584  0.182961  2.211320  1.805225  1.480693  1.445918  0.361411 -0.356336   \n",
       "50585  0.357590  0.946273  1.268209  0.283762  3.105134  2.102657 -0.898391   \n",
       "50586  0.230199  1.042811  1.529540  0.703689  2.225735  1.142767 -0.695822   \n",
       "50587 -0.028394  1.833864  1.221431  0.805910  1.913851  0.948037 -0.490566   \n",
       "50588 -0.438666  2.606170  1.740339  0.983071  1.565430  1.317871 -0.903871   \n",
       "\n",
       "            7         8         9    ...       118       119       120  \\\n",
       "0     -0.461428 -0.637427 -0.611985  ... -1.380926  1.291588  3.231824   \n",
       "1     -1.089974 -0.566921 -0.317467  ... -0.387416  1.024946  2.244558   \n",
       "2     -0.072700 -0.512950  0.287123  ... -0.352306  1.462230  0.445271   \n",
       "3      0.972724 -1.133452 -1.039080  ...  0.089598 -0.750147  1.849371   \n",
       "4      2.912364 -0.884565  0.425860  ...  0.362923  0.245477  0.673627   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "50584  0.911873 -0.327498  1.320318  ...  1.688902  1.070147  1.047303   \n",
       "50585  1.466275 -0.227159  1.035146  ...  0.801446  0.807440  1.796256   \n",
       "50586  1.781696  0.190462  0.516006  ...  0.951339  0.521067  1.875985   \n",
       "50587  1.633394 -0.326149 -0.071438  ...  1.729324  1.294404  1.559716   \n",
       "50588  0.867624 -0.268109  2.056823  ...  1.597416  1.730113  1.182213   \n",
       "\n",
       "            121       122       123       124       125       126       127  \n",
       "0     -1.089608 -0.278298 -1.629229  0.745970 -1.162513 -0.956747 -0.602077  \n",
       "1     -0.733578 -1.347370 -1.255779 -0.328468  2.809101 -0.764077  2.894966  \n",
       "2     -0.385861 -0.960686 -0.577423 -0.462516 -1.020753 -0.526767  2.492939  \n",
       "3     -1.229478 -1.216052 -0.970517 -0.432945  1.118007 -0.068015 -0.280603  \n",
       "4     -1.123325 -0.799653 -1.161224  0.962933 -0.421276 -1.176906  0.747134  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "50584  1.543294  0.785552  1.175742  1.371868  1.143094  0.849715  0.742523  \n",
       "50585  0.166234 -0.323527  0.401960  1.001705  0.834673  0.467638  0.175889  \n",
       "50586  0.725284  0.020722  0.748264  0.863091  0.745802  0.791014  0.215019  \n",
       "50587  1.259737  0.920063  1.105738  0.657258  1.619102  0.777057  0.887049  \n",
       "50588  1.604889  1.089531  0.878835  2.072124  1.072981  0.623154  1.233232  \n",
       "\n",
       "[50589 rows x 128 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_sweetnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models/embeddings/RoBERTa', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(embed:np.ndarray, data:pd.DataFrame, hue:str, limit:int = 5, errors=None, seed=42):\n",
    "\n",
    "    assert(hue in data.columns)\n",
    "    assert(embed.shape[0] == data.shape[0])\n",
    "    if errors:\n",
    "        data = data[~data['glycan'].isin(errors)].reset_index(drop=True)\n",
    "    \n",
    "    tsne_embeds = TSNE(n_components=2, random_state=seed).fit_transform(embed)\n",
    "    df_tsne = pd.DataFrame(tsne_embeds, columns=['x', 'y'])  \n",
    "    df_tsne['glycan'] = data['glycan'].tolist()\n",
    "\n",
    "    # Select the most relevant categories to see the clusters\n",
    "    df_tsne['hue'] = data[hue].tolist()\n",
    "    df_tsne = df_tsne.explode('hue').drop_duplicates(subset=['glycan', 'hue']).reset_index(drop=True)\n",
    "    top_hues = df_tsne['hue'].value_counts().nlargest(limit).index.tolist()\n",
    "    df_tsne = df_tsne[df_tsne['hue'].isin(top_hues)].reset_index(drop=True)\n",
    "\n",
    "    sns.set_theme(rc = {'figure.figsize':(10, 10)}, font_scale=2)\n",
    "    fig = sns.scatterplot(data=df_tsne, x='x', y='y', hue=hue, palette='colorblind', s=40, rasterized=True)\n",
    "    fig.set_title('TSNE of Glycan Embeddings')\n",
    "\n",
    "    return tsne_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(embeddings_roberta, data, hue='Kingdom', limit=5, errors=errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
